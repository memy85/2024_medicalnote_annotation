{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'text'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessage(content='You are a helpful assistant.'), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_PATH = \"/data/data_user/public_models/Llama_2_hf/Llama-2-7b-hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n",
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, LlamaForCausalLM, LlamaTokenizerFast\n",
    "\n",
    "# first we need to load the model as we did in huggingface\n",
    "model = LlamaForCausalLM.from_pretrained(LLAMA2_PATH)\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(LLAMA2_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# first define a huggingface pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, model_kwargs={\"temperature\":0},\n",
    "    batch_size=4, device=1,\n",
    ")\n",
    "# pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "# and then use HuggingFacePipeline from langchain\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'text'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessage(content='You are a helpful assistant.'), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First, we need to know what is EEG.\n",
      "\n",
      "EEG is the recording of electrical activity of the brain.\n",
      "\n",
      "Now, we need to know what is electroencephalography.\n",
      "\n",
      "Electroencephalography is the recording of electrical activity of the brain.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Instruction : {instruction}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "instruction = \"You will be given a question. Try to answer the question briefly.\"\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "answer = chain.invoke({\"question\": question, \"instruction\": instruction})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['instruction', 'question'], template=\"\\nInstruction : {instruction}\\n\\nQuestion: {question}\\n\\nAnswer: Let's think step by step.\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFirst, we need to know what is EEG.\\n\\nEEG is the recording of electrical activity of the brain.\\n\\nNow, we need to know what is electroencephalography.\\n\\nElectroencephalography is the recording of electrical activity of the brain.\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First, we need to know what is the number 0 in french.\n",
      "\n",
      "Second, we need to know what is the number 0 in english.\n",
      "\n",
      "Third, we need to know what is the number 0 in french.\n",
      "\n",
      "Fourth, we need to\n",
      "\n",
      "\n",
      "1. First, we need to know what is the number 1 in french.\n",
      "\n",
      "2. Then, we need to know what is the number 1 in english.\n",
      "\n",
      "3. Finally, we need to know what is the number 1 in french.\n",
      "\n",
      "4.\n",
      "\n",
      "\n",
      "First, we need to know the number 2 in french.\n",
      "\n",
      "Second, we need to know the number 2 in english.\n",
      "\n",
      "Third, we need to know the number 2 in french.\n",
      "\n",
      "Fourth, we need to know the number 2 in\n",
      "\n",
      "\n",
      "First, we need to know the number 3 in french.\n",
      "\n",
      "Second, we need to know the number 3 in english.\n",
      "\n",
      "Third, we need to know the number 3 in german.\n",
      "\n",
      "Fourth, we need to know the number 3 in\n"
     ]
    }
   ],
   "source": [
    "gpu_chain = prompt | hf\n",
    "\n",
    "questions = []\n",
    "for i in range(4):\n",
    "    questions.append({\"question\": f\"What is the number {i} in french?\", \"instruction\" : instruction})\n",
    "\n",
    "answers = gpu_chain.batch(questions)\n",
    "for answer in answers:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"\n",
    "### Instruction : {instruction}\n",
    "\n",
    "### Question: {question}\n",
    "\n",
    "### Answer: Let's think step by step \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"instruction\", \"question\"])\n",
    "\n",
    "chain = prompt | hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['instruction', 'question'], template=\"\\n### Instruction : {instruction}\\n\\n### Question: {question}\\n\\n### Answer: Let's think step by step \")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "2. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "3. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "4. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "5. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "6. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "7. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "8. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "9. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "10. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "11. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "12. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "13. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "14. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "15. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "16. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "17. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "18. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "19. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "20. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "21. We have to find the winner of the FIFA World Cup in the year 1994.\n",
      "22. We have to find the winner of the FIFA World Cup in\n"
     ]
    }
   ],
   "source": [
    "answer = chain.invoke({\"question\" : question, \"instruction\" : \"You are a smart agent. Briefly answer the Question\"})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n### 1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
