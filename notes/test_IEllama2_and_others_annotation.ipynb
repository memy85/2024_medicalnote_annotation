{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import os\n",
    "from utils import *\n",
    "config = load_config()\n",
    "\n",
    "\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n",
    "\n",
    "LLAMA_PATH = \"/home/htran/generation/biomed_instruct/models/llama_7b_lora/checkpoint-970\"\n",
    "LLAMA2_PATH = \"/home/htran/generation/biomed_instruct/models/llama_2_7b_all_instructions/checkpoint-975\"\n",
    "IE_LLAMA_PATH = '/home/htran/generation/biomed_instruct/models/llama_2_7b_IE_updated_category/checkpoint-330'\n",
    "\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", LLAMA2_PATH, max_new_tokens=128, device_map=\"auto\")\n",
    "# hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there a ~ e styles of instructions.\n",
      "The instruction tells the model to extract important terms.\n",
      "\n",
      "{'a': 'Extract and list the main health concerns or conditions mentioned in the medical note, making sure the terms are patient-friendly and easily understandable.', 'b': \"Provide a clear summary of any treatment plans or medications prescribed in the note, including their purposes and how they should be administered, in layman's terms.\", 'c': 'Identify and explain any recommended follow-up actions or appointments, ensuring that the instructions are straightforward and actionable for the patient.', 'd': \"Interpret and summarize any test results or diagnostics from the note, using simple language to convey what these results mean for the patient's health.\", 'e': 'Extract any preventative measures or lifestyle recommendations given in the medical note, presenting them in an easy-to-follow format for the patient.', 'hieu': \"Translate the medical jargon in the provided sentence into layman's terms.\"}\n"
     ]
    }
   ],
   "source": [
    "template = config.template('instructiontune')\n",
    "instructions = config.template('instructions')\n",
    "\n",
    "print(\"there a ~ e styles of instructions.\\nThe instruction tells the model to extract important terms.\\n\")\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================================================== Load dataset\n",
    "- load dataset : filtered notes, annotation info table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "annotation_info_table = pd.read_pickle(\"../data/processed/annotation_info_table.pkl\")\n",
    "filtered_notes = pd.read_pickle(\"../data/processed/filtered_notes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note(noteid) :\n",
    "    text = filtered_notes[filtered_notes['noteid'] == noteid]['text']\n",
    "    annotations = annotation_info_table[annotation_info_table['noteid'] == noteid]['concept']\n",
    "    annotations = annotations.tolist()\n",
    "    return text, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>noteid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report38874.txt</td>\n",
       "      <td>F/u on Osteoarthritis, chronic pain, HTN, Depr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report70734.txt</td>\n",
       "      <td>His diabetes has been under control, recently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report72683.txt</td>\n",
       "      <td>A. Status post resection on XXXXX for T3 N2 M0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report84657.txt</td>\n",
       "      <td>This is a followup infectious disease clinic f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report88311.txt</td>\n",
       "      <td>1. Erosive seropositive rheumatoid arthritis t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                         noteid  \\\n",
       "2   liver_failure  liver_failure.report38874.txt   \n",
       "9   liver_failure  liver_failure.report70734.txt   \n",
       "10  liver_failure  liver_failure.report72683.txt   \n",
       "13  liver_failure  liver_failure.report84657.txt   \n",
       "15  liver_failure  liver_failure.report88311.txt   \n",
       "\n",
       "                                                 text  \n",
       "2   F/u on Osteoarthritis, chronic pain, HTN, Depr...  \n",
       "9   His diabetes has been under control, recently ...  \n",
       "10  A. Status post resection on XXXXX for T3 N2 M0...  \n",
       "13  This is a followup infectious disease clinic f...  \n",
       "15  1. Erosive seropositive rheumatoid arthritis t...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_sample = filtered_notes.loc[2]['text']\n",
    "# print(note_sample)\n",
    "\n",
    "filtered_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "      <th>concept</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>noteid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer.report20</td>\n",
       "      <td>non-Hodgkin's lymphoma</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancer.report20.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancer.report20</td>\n",
       "      <td>non-Hodgkin's large B-cell lymphoma</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancer.report20.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancer.report20</td>\n",
       "      <td>vincristine</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancer.report20.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cancer.report20</td>\n",
       "      <td>cyclophosphamide</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancer.report20.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cancer.report20</td>\n",
       "      <td>folinic acid</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancer.report20.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              note                              concept    A   B    C  \\\n",
       "0  cancer.report20               non-Hodgkin's lymphoma  1.1 NaN  NaN   \n",
       "1  cancer.report20  non-Hodgkin's large B-cell lymphoma  1.1 NaN  NaN   \n",
       "2  cancer.report20                          vincristine    2 NaN  NaN   \n",
       "3  cancer.report20                     cyclophosphamide    2 NaN  NaN   \n",
       "4  cancer.report20                         folinic acid    2 NaN  NaN   \n",
       "\n",
       "                noteid  \n",
       "0  cancer.report20.txt  \n",
       "1  cancer.report20.txt  \n",
       "2  cancer.report20.txt  \n",
       "3  cancer.report20.txt  \n",
       "4  cancer.report20.txt  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_info_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_info_table['concept_modified'] = annotation_info_table.concept.apply(lambda x : str(x) + \", \")\n",
    "annotations = annotation_info_table.groupby('noteid')['concept_modified'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>noteid</th>\n",
       "      <th>text</th>\n",
       "      <th>concept_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report38874.txt</td>\n",
       "      <td>F/u on Osteoarthritis, chronic pain, HTN, Depr...</td>\n",
       "      <td>generalized OA, Diffuse OA, Gabapentin, naproxen,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report70734.txt</td>\n",
       "      <td>His diabetes has been under control, recently ...</td>\n",
       "      <td>diabetes, metformin, Lantus, Humalog, A1c, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report72683.txt</td>\n",
       "      <td>A. Status post resection on XXXXX for T3 N2 M0...</td>\n",
       "      <td>Polycythemia, peripheral vascular, colon cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report84657.txt</td>\n",
       "      <td>This is a followup infectious disease clinic f...</td>\n",
       "      <td>osteodiscitis, epidural phlegmon/abscess, MSSA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report88311.txt</td>\n",
       "      <td>1. Erosive seropositive rheumatoid arthritis t...</td>\n",
       "      <td>rheumatoid arthritis, Ankylosing spondylitis, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report100741.txt</td>\n",
       "      <td>I had the pleasure of seeing your patient, Mrs...</td>\n",
       "      <td>carpal tunnel syndrome, Tramadol, hypercholest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report113518.txt</td>\n",
       "      <td>Mr. name returns for followup of rheumatoid ar...</td>\n",
       "      <td>rheumatoid arthritis, Osteoarthritis, Humira i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report114863.txt</td>\n",
       "      <td>I had the pleasure of seeing Mr. name name in ...</td>\n",
       "      <td>deformed pylorus and duodenal region, Intussus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report119233.txt</td>\n",
       "      <td>I had the pleasure of seeing your patient, nam...</td>\n",
       "      <td>end-stage liver disease, alcoholic cirrhosis, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>liver_failure</td>\n",
       "      <td>liver_failure.report120088.txt</td>\n",
       "      <td>The patient is an 18-year-old woman with a his...</td>\n",
       "      <td>cholecystitis, cholelithiasis, fatty liver, ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>copd</td>\n",
       "      <td>copd.report100446.txt</td>\n",
       "      <td>The patient presents for evaluation of her med...</td>\n",
       "      <td>Osteoarthritis, Obesity, Tobacco abuse, COPD, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>copd</td>\n",
       "      <td>copd.report101687.txt</td>\n",
       "      <td>Mr. name was seen today on xxx during multidis...</td>\n",
       "      <td>kidney transplantation, diabetes type 2, hyper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>copd</td>\n",
       "      <td>copd.report26318.txt</td>\n",
       "      <td>F/u on HTN, CAD, GERD, DM, OA, BPH\\n\\nThe pt h...</td>\n",
       "      <td>COPD, bronchoscopy,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>copd</td>\n",
       "      <td>copd.report33401.txt</td>\n",
       "      <td>I saw Mr. name and his wife in the liver trans...</td>\n",
       "      <td>liver transplant, alcohol, hepatitis C related...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>copd</td>\n",
       "      <td>copd.report34013.txt</td>\n",
       "      <td>Depression. Much worse since August. Daughter ...</td>\n",
       "      <td>Depression, citalopram, fluoxetine, arrythmia ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                          noteid  \\\n",
       "0   liver_failure   liver_failure.report38874.txt   \n",
       "1   liver_failure   liver_failure.report70734.txt   \n",
       "2   liver_failure   liver_failure.report72683.txt   \n",
       "3   liver_failure   liver_failure.report84657.txt   \n",
       "4   liver_failure   liver_failure.report88311.txt   \n",
       "5   liver_failure  liver_failure.report100741.txt   \n",
       "6   liver_failure  liver_failure.report113518.txt   \n",
       "7   liver_failure  liver_failure.report114863.txt   \n",
       "8   liver_failure  liver_failure.report119233.txt   \n",
       "9   liver_failure  liver_failure.report120088.txt   \n",
       "10           copd           copd.report100446.txt   \n",
       "11           copd           copd.report101687.txt   \n",
       "12           copd            copd.report26318.txt   \n",
       "13           copd            copd.report33401.txt   \n",
       "14           copd            copd.report34013.txt   \n",
       "\n",
       "                                                 text  \\\n",
       "0   F/u on Osteoarthritis, chronic pain, HTN, Depr...   \n",
       "1   His diabetes has been under control, recently ...   \n",
       "2   A. Status post resection on XXXXX for T3 N2 M0...   \n",
       "3   This is a followup infectious disease clinic f...   \n",
       "4   1. Erosive seropositive rheumatoid arthritis t...   \n",
       "5   I had the pleasure of seeing your patient, Mrs...   \n",
       "6   Mr. name returns for followup of rheumatoid ar...   \n",
       "7   I had the pleasure of seeing Mr. name name in ...   \n",
       "8   I had the pleasure of seeing your patient, nam...   \n",
       "9   The patient is an 18-year-old woman with a his...   \n",
       "10  The patient presents for evaluation of her med...   \n",
       "11  Mr. name was seen today on xxx during multidis...   \n",
       "12  F/u on HTN, CAD, GERD, DM, OA, BPH\\n\\nThe pt h...   \n",
       "13  I saw Mr. name and his wife in the liver trans...   \n",
       "14  Depression. Much worse since August. Daughter ...   \n",
       "\n",
       "                                     concept_modified  \n",
       "0   generalized OA, Diffuse OA, Gabapentin, naproxen,  \n",
       "1   diabetes, metformin, Lantus, Humalog, A1c, per...  \n",
       "2   Polycythemia, peripheral vascular, colon cance...  \n",
       "3   osteodiscitis, epidural phlegmon/abscess, MSSA...  \n",
       "4   rheumatoid arthritis, Ankylosing spondylitis, ...  \n",
       "5   carpal tunnel syndrome, Tramadol, hypercholest...  \n",
       "6   rheumatoid arthritis, Osteoarthritis, Humira i...  \n",
       "7   deformed pylorus and duodenal region, Intussus...  \n",
       "8   end-stage liver disease, alcoholic cirrhosis, ...  \n",
       "9   cholecystitis, cholelithiasis, fatty liver, ab...  \n",
       "10  Osteoarthritis, Obesity, Tobacco abuse, COPD, ...  \n",
       "11  kidney transplantation, diabetes type 2, hyper...  \n",
       "12                                COPD, bronchoscopy,  \n",
       "13  liver transplant, alcohol, hepatitis C related...  \n",
       "14  Depression, citalopram, fluoxetine, arrythmia ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the notes\n",
    "merged_notes = filtered_notes.merge(annotations, on='noteid')\n",
    "merged_notes['concept_modified'] = merged_notes['concept_modified'].str.strip()\n",
    "merged_notes.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEID = \"cancer.report20.txt\"\n",
    "\n",
    "note, concepts = get_note(NOTEID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['category', 'noteid', 'text', 'concept_modified'],\n",
       "    num_rows: 51\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets build a dataset \n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(merged_notes)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============================================ Zero Shot Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.66s/it]\n",
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# first test llama & llama2\n",
    "# model = pipeline(\"text-generation\", LLAMA2_PATH, max_new_tokens=128, device_map=\"auto\")\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\"\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = \"0,1,2,3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_PAD_TOKEN = \"<pad>\"\n",
    "DEFAULT_EOS_TOKEN= \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "max_target_length = 2048\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA2_PATH, cache_dir=None)\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA2_PATH, cache_dir=None)\n",
    "tokenizer.add_special_tokens({\n",
    "    'eos_token' : DEFAULT_EOS_TOKEN,\n",
    "    'bos_token' : DEFAULT_BOS_TOKEN,\n",
    "    'unk_token' : DEFAULT_UNK_TOKEN,\n",
    "})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = load_config()\n",
    "instruction = config.template('instructions')\n",
    "i1 = instruction['a']\n",
    "i2 = instruction['hieu']\n",
    "\n",
    "zeroshot_template = config.template('zeroshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instruction :  {instruction}\\n\\nContext :  {context}\\n\\nResponse : \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51/51 [00:00<00:00, 8353.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def process_texts(samples) :\n",
    "\n",
    "    texts = samples['text']\n",
    "    formated_texts = []\n",
    "    for text in texts :\n",
    "        new_text = zeroshot_template.format(\n",
    "            instruction = i1,\n",
    "            context = text\n",
    "        )\n",
    "        formated_texts.append(new_text)\n",
    "    \n",
    "    return {\"questions\" : formated_texts}\n",
    "\n",
    "processed_dataset = dataset.map(process_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4564,\n",
       " 6234,\n",
       " 5430,\n",
       " 5521,\n",
       " 4744,\n",
       " 5926,\n",
       " 4071,\n",
       " 6254,\n",
       " 5266,\n",
       " 5412,\n",
       " 6461,\n",
       " 5290,\n",
       " 5328,\n",
       " 6162,\n",
       " 3023,\n",
       " 5556,\n",
       " 4698,\n",
       " 5486,\n",
       " 6255,\n",
       " 4233,\n",
       " 5770,\n",
       " 5924,\n",
       " 3654,\n",
       " 3970,\n",
       " 5922,\n",
       " 3576,\n",
       " 3766,\n",
       " 6457,\n",
       " 6107,\n",
       " 6469,\n",
       " 4426,\n",
       " 5019,\n",
       " 5141,\n",
       " 4704,\n",
       " 6037,\n",
       " 4590,\n",
       " 5688,\n",
       " 5249,\n",
       " 5158,\n",
       " 5983,\n",
       " 5800,\n",
       " 3026,\n",
       " 4573,\n",
       " 5212,\n",
       " 4494,\n",
       " 4748,\n",
       " 5306,\n",
       " 6309,\n",
       " 5903,\n",
       " 5264,\n",
       " 6254]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x : len(x), processed_dataset['questions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the texts\n",
    "tokenized_texts = tokenizer(processed_dataset['questions'], \n",
    "                            return_tensors='pt', \n",
    "                            max_length=4000,\n",
    "                            truncation='only_first',\n",
    "                            padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacty of 39.42 GiB of which 425.06 MiB is free. Process 3674365 has 31.26 GiB memory in use. Process 2573385 has 1.18 GiB memory in use. Including non-PyTorch memory, this process has 6.57 GiB memory in use. Of the allocated memory 5.63 GiB is allocated by PyTorch, and 11.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad() :\n\u001b[0;32m----> 3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/utils.py:1718\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1702\u001b[0m         input_ids,\n\u001b[1;32m   1703\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1715\u001b[0m     )\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/utils.py:2579\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2579\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1181\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1178\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1033\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# output_attentions=True can not be supported when using SDPA, and we fall back on\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# the manual implementation that requires a 4D causal mask in all cases.\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_causal_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# 4d mask is passed through the layers\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask(\n\u001b[1;32m   1042\u001b[0m         attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n\u001b[1;32m   1043\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:372\u001b[0m, in \u001b[0;36m_prepare_4d_causal_attention_mask_for_sdpa\u001b[0;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m    368\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m attn_mask_converter\u001b[38;5;241m.\u001b[39mto_causal_4d(\n\u001b[1;32m    369\u001b[0m         input_shape[\u001b[38;5;241m0\u001b[39m], input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], key_value_length, dtype\u001b[38;5;241m=\u001b[39minputs_embeds\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39minputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    370\u001b[0m     )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     expanded_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_4d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:132\u001b[0m, in \u001b[0;36mAttentionMaskConverter.to_4d\u001b[0;34m(self, attention_mask_2d, query_length, dtype, key_value_length)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSliding window is currently only implemented for causal masking\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m expanded_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    133\u001b[0m     attention_mask_2d\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m causal_4d_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     expanded_attn_mask \u001b[38;5;241m=\u001b[39m causal_4d_mask\u001b[38;5;241m.\u001b[39mmasked_fill(expanded_attn_mask\u001b[38;5;241m.\u001b[39mbool(), torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin)\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/modeling_attn_mask_utils.py:183\u001b[0m, in \u001b[0;36mAttentionMaskConverter._expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    179\u001b[0m tgt_len \u001b[38;5;241m=\u001b[39m tgt_len \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m src_len\n\u001b[1;32m    181\u001b[0m expanded_mask \u001b[38;5;241m=\u001b[39m mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mexpand(bsz, \u001b[38;5;241m1\u001b[39m, tgt_len, src_len)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[0;32m--> 183\u001b[0m inverted_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexpanded_mask\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inverted_mask\u001b[38;5;241m.\u001b[39mmasked_fill(inverted_mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool), torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin)\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mna/lib/python3.9/site-packages/torch/_tensor.py:909\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacty of 39.42 GiB of which 425.06 MiB is free. Process 3674365 has 31.26 GiB memory in use. Process 2573385 has 1.18 GiB memory in use. Including non-PyTorch memory, this process has 6.57 GiB memory in use. Of the allocated memory 5.63 GiB is allocated by PyTorch, and 11.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad() :\n",
    "    output = model.generate(**tokenized_texts, max_new_tokens = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now decode the outputs\n",
    "start_of_generate_index = tokenized_texts.input_ids.shape[1]\n",
    "pred_output = tokenizer.batch_decode(output[:, start_of_generate_index:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. generalized OA 2. D',\n",
       " 'Љ1. Diabetes mellitus',\n",
       " 'Љ Status post resection on T3 N2',\n",
       " 'Љ4-L5 osteodiscitis',\n",
       " '. Erosive seropositive rhe',\n",
       " 'Љ Hand weakness \\nЉ Frequent falls',\n",
       " '. erosive rheumatoid ar',\n",
       " 'Љаптоманија, пеп',\n",
       " 'Љ Alcoholic cirrhosis ',\n",
       " 'ЉРђРЅР°Р№',\n",
       " 'Љ1. Osteoarthritis ',\n",
       " 'ЉћЂЉћЂЉћЂЉ',\n",
       " '\\nContext :  F/u on CAD',\n",
       " 'Љиврплт: \\n',\n",
       " '. Depression \\n. Ankle pain',\n",
       " 'Љ Coronary artery disease, status post',\n",
       " 'Љ Diabetes: Glyburide ',\n",
       " 'Љајнт: 74-',\n",
       " 'Љ Coronary artery disease \\nЉ',\n",
       " 'Љ Methotrexate/sul',\n",
       " 'Љ Hypertension \\nЉ Alzheimer',\n",
       " '. \\n. \\n. \\n.',\n",
       " 'Љ Idiopathic pulmonary hemos',\n",
       " '. \\n. \\n. \\n.',\n",
       " 'Љ Hypertension, complicated with possible neph',\n",
       " '1. Lymphoma 2.',\n",
       " '. malignant large B cell diffuse l',\n",
       " 'Љ\\n',\n",
       " 'Љ Methotrexate 17',\n",
       " '1. Rheumatoid arthrit',\n",
       " 'Љивни болест\\n\\nContext',\n",
       " 'Љ Graft: Well engrafted, off',\n",
       " 'Љ78-year-old male with a',\n",
       " 'Љејѕѕеd ',\n",
       " 'Љамфом, степен III,',\n",
       " 'ЉРђРЅР°Рј',\n",
       " 'Љ Hypertension \\nЉ Atrial f',\n",
       " '\\\\\\\\n1. Diabetes Mellit',\n",
       " 'Љ Wellness check \\nЉ DM \\n',\n",
       " 'ЉAnxiety, functional dyspeps',\n",
       " '1. Diabetes mellitus',\n",
       " '1. Hyperlipidemia 2',\n",
       " 'ЉDiabetes: does not check levels at',\n",
       " 'ЉРђРЅР°Р№',\n",
       " 'Љ Alprazolam 0.2',\n",
       " '. Nonischemic cardiomyopath',\n",
       " 'ЉLightheadedness, abdominal pain',\n",
       " 'Љајтнaмa Љ',\n",
       " 'Љ Type 2 diabetes mellit',\n",
       " '. Health maintenance issues. \\n. Gastro',\n",
       " 'Љауреат: \\n1.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============================================ Few Shot Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]\n",
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/wjang/miniconda3/envs/mna/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# first test llama & llama2\n",
    "# model = pipeline(\"text-generation\", LLAMA2_PATH, max_new_tokens=128, device_map=\"auto\")\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\"\n",
    "\n",
    "DEFAULT_PAD_TOKEN = \"<pad>\"\n",
    "DEFAULT_EOS_TOKEN= \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "max_target_length = 4096\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA2_PATH, cache_dir=None)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA2_PATH, cache_dir=None)\n",
    "tokenizer.add_special_tokens({\n",
    "    'eos_token' : DEFAULT_EOS_TOKEN,\n",
    "    'bos_token' : DEFAULT_BOS_TOKEN,\n",
    "    'unk_token' : DEFAULT_UNK_TOKEN,\n",
    "})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = load_config()\n",
    "instruction = config.template('instructions')\n",
    "i1 = instruction['a']\n",
    "i2 = instruction['hieu']\n",
    "\n",
    "fewshot_template = config.template('fewshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Instruction :  {instruction}\\n\\nContext :  F/u on Osteoarthritis, chronic pain, HTN, Depression  The pt has generalized OA, in his shoulders, ankle and knees, the pain is about XXX in severity, worse with cold weather and activity, was on narocitcs in the past, then he stopped and went to ETOH for about 1.5 yrs, he went through detox, has not drank in the last month. He is currnetly on naproxen with no effect, was never tried on neuromodulators.  The pt also has HTN with hx of Afib that was single episode during an episode of pancreatitis, No CP, No SOB, No PNDs, No Orthopnea, No Edema, No Dizziness/Syncope, No Palpitations.  The pt also has depression, on wellbutrin, he's been on it for a couple of yrs with good results, no hx of suicidal or homocidal thoughts, No Depression, No Anxiety, Positive Sleep disturbance, No Sexual dysfunction.  The pt also has hx of Hep C, diagnosed in XXX, was treated in XXX, and since then he's been doing very well, he also had recurrent episodes of pancreatitis, ETOH related, No Nausea/Vomiting, No Dyspepsia, No Pain, No early satiety, No Diarrhea, No Constipation, No Cramps, No Hematochezia, No Melena, No GERD, of note the pt will need a colonoscopy XXXX for hx of colon polyps.  Review of Systems:  General:  No Wt loss, No Fatigue, No Fevers, No Chills, No Night sweats  Eyes:  No Vision change/loss, No Headaches, No Discharge, No Pain \\nAssessment and Plan:  1) Diffuse OA with chronic pain  I have discussed this with the pt in great detail, the pt had multipe surgeries and has OA in multiple sites, I don't doubt that he has pain, but on the other hand, he has issues with substance abuse in the past, ETOh being the most recent, so using narcotics for his pain is extremely high risk.  So at this point we agreed to try the neuromodulators, I have explained to the pt how they work, and that they will need to be used regularly for at least 2 months for us to assess if they are helping for his pain. He agreed and understood  Will start Gabapentin 300 mg QHS to be increased to TID over the next 3 weeks.  Continue naproxen for the time being for his pain  2) HTN  Check CBC, CMP, FLP, TSH, UA  Continue current meds  3) Depression  Insomnia is his problem  name Trazodone 50 mg QHS  4) Hx of Hep C  Check titers  5) Hx of Afib, single episode  Start ASA 81 mg daily  6) Hx of colon polyps  Will need a colonoscopy in summer XXXX  7) Hepatomegaly on exam  Schedule Abd u/s  Follow up: 4 months  ||||END_OF_RECORD \\n\\nResponse :  1. generalized OA 2. Diffuse OA 3. Gabapentin 4. naproxen\\n\\nContext :  {context}\\n\\nResponse : \\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshot_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51/51 [00:00<00:00, 1724.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def process_texts(samples) :\n",
    "\n",
    "    example = config.template(\"fewshot_example\")\n",
    "    texts = samples['text']\n",
    "    formated_texts = []\n",
    "    for text in texts :\n",
    "        contexts = example.format(instruction = i1)\n",
    "        new_text = fewshot_template.format(\n",
    "            instruction = i1,\n",
    "            context = text\n",
    "        )\n",
    "        formated_texts.append(new_text)\n",
    "    \n",
    "    return {\"contexts\": [contexts]*len(formated_texts), \"questions\" : formated_texts}\n",
    "\n",
    "\n",
    "processed_dataset = dataset.map(process_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7077,\n",
       " 8747,\n",
       " 7943,\n",
       " 8034,\n",
       " 7257,\n",
       " 8439,\n",
       " 6584,\n",
       " 8767,\n",
       " 7779,\n",
       " 7925,\n",
       " 8974,\n",
       " 7803,\n",
       " 7841,\n",
       " 8675,\n",
       " 5536,\n",
       " 8069,\n",
       " 7211,\n",
       " 7999,\n",
       " 8768,\n",
       " 6746,\n",
       " 8283,\n",
       " 8437,\n",
       " 6167,\n",
       " 6483,\n",
       " 8435,\n",
       " 6089,\n",
       " 6279,\n",
       " 8970,\n",
       " 8620,\n",
       " 8982,\n",
       " 6939,\n",
       " 7532,\n",
       " 7654,\n",
       " 7217,\n",
       " 8550,\n",
       " 7103,\n",
       " 8201,\n",
       " 7762,\n",
       " 7671,\n",
       " 8496,\n",
       " 8313,\n",
       " 5539,\n",
       " 7086,\n",
       " 7725,\n",
       " 7007,\n",
       " 7261,\n",
       " 7819,\n",
       " 8822,\n",
       " 8416,\n",
       " 7777,\n",
       " 8767]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x : len(x), processed_dataset['questions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the texts\n",
    "tokenized_texts = tokenizer(processed_dataset['contexts'], \n",
    "                            processed_dataset['questions'], \n",
    "                            return_tensors='pt', \n",
    "                            padding=True,\n",
    "                            max_length=4000,\n",
    "                            truncation='only_first').to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad() :\n",
    "    output = model.generate(**tokenized_texts, max_new_tokens = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now decode the outputs\n",
    "start_of_generate_index = tokenized_texts.input_ids.shape[1]\n",
    "pred_output = tokenizer.batch_decode(output[:, start_of_generate_index:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nInstruction :  Extract and list the',\n",
       " '\\nInstruction : \\nExtract and list',\n",
       " '. \\n. \\n. \\n.',\n",
       " '. L4-L5 osteodisc',\n",
       " '. Erosive seropositive rhe',\n",
       " '. \"Instruction\": \"Extract and list',\n",
       " '. \"Instruction\": \"Extract and list',\n",
       " '[{\"type\": \"question\", \"question\":',\n",
       " '. \"Alcoholic cirrhosis',\n",
       " '. \"Patient-friendly and easily understand',\n",
       " '. \\n. \\n. \\n.',\n",
       " '. \"Instruction\": \"Extract and list',\n",
       " '\\nInstruction :  Extract and list the',\n",
       " '. \"Patient\": \"Mr. name\",',\n",
       " '. Esophagitis, presented with GI',\n",
       " '. \\n. \\n. \\n.',\n",
       " '.Diabetes: I strongly emphasized on',\n",
       " '. \"Patient_ID\": \"12',\n",
       " '. Coronary artery disease: Overall',\n",
       " '. \\n. \\n. \\n.',\n",
       " 'at 2021-02-',\n",
       " '. \"Patient_Name\": \"Ms',\n",
       " ', \"Idiopathic pulmonary hem',\n",
       " '. Extract and list the main health concerns or',\n",
       " '. Hypertension, complicated with possible neph',\n",
       " '\\nInstruction :  Extract and list the',\n",
       " '. Left back and left-sided pain\\n',\n",
       " '.000000000',\n",
       " '. \\n. \\n. \\n.',\n",
       " '{\"status\":\"success\",\"data\":{\"id\":1',\n",
       " '. \\n. \\n. \\n.',\n",
       " '. \"Graft\": \"Well engrafted',\n",
       " '. \\n. \\n. \\n.',\n",
       " '. \\n',\n",
       " '. \\n. \\n. \\n.',\n",
       " '. \\n. \\n. \\n.',\n",
       " '[{\"type\": \"question\", \"question\":',\n",
       " '\\\\nInstruction :  Extract and list',\n",
       " '\\nInstruction :  Extract and list the',\n",
       " '. \\n. \\n. \\n.',\n",
       " '\\n \\n \\n \\n \\n',\n",
       " '\\nInstruction :  Extract and list',\n",
       " '\\nInstruction :  Extract and list the',\n",
       " ',Stage III chronic kidney disease. This',\n",
       " '. \\n. \\n. \\n.',\n",
       " '. \"Mrs. name is a lovely',\n",
       " '. \\n. \\n. \\n.',\n",
       " '[{\"name\":\"Health concerns\",\"value\":\"',\n",
       " '\\n \\n \\n \\n \\n',\n",
       " '. Health maintenance issues: Blood work, exercise,',\n",
       " '. \"Patient\": \"Mr. name\",']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
